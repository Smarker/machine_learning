{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Typically, neural networks are layered. However, you can have a single layer neural network. If the neural network had multiple layers it would be composed of:\n",
    "\n",
    "* **Input layer** - takes input data, no computations at this layer, data is passed onto the hidden layer (or output layer if there's no hidden layer)\n",
    "* **Hidden layer(s)** - optional layer, performs computations on the data and transform the data\n",
    "* **Output layer** - outputs data, performs computations on the data\n",
    "\n",
    "![neural_net_basic](https://user-images.githubusercontent.com/7232635/45890124-2d6f2880-bd90-11e8-9511-747f526fd911.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "The most basic unit of a neural network is the **perceptron**. A perceptron defines a **[hyperplane](https://en.wikipedia.org/wiki/Hyperplane)**, $y$, in **$(n+1)$-dimensional space**:\n",
    "\n",
    "$$ y = w_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n $$\n",
    "\n",
    "### Hyperplane in $\\mathbb{R}^2$\n",
    "\n",
    "Since we are operating in $\\mathbb{R}^2$, then the hyperplane would be a line.\n",
    "\n",
    "![perceptron](https://user-images.githubusercontent.com/7232635/45838302-67342680-bcdf-11e8-9c7f-ed94fd9e9e89.png)\n",
    "\n",
    "### Hyperplane in $\\mathbb{R}^3$\n",
    "\n",
    "Since we are operating in $\\mathbb{R}^3$, then the hyperplane would be a plane (in green).\n",
    "\n",
    "![3dhyperplane](https://user-images.githubusercontent.com/7232635/45893393-12ed7d00-bd99-11e8-9cc8-ca050ed9bd12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Anatomy\n",
    "\n",
    "If the input data are **[linearly separable](https://en.wikipedia.org/wiki/Linear_separability)**, a _single_ perceptron could model the split between the points with a hyperplane. In the case of non-linearly separable points, _multiple_ perceptrons are needed to model that complexity. We will see this in our next example which will use multiple perceptrons to model the **XOR** boolean operator.\n",
    "\n",
    "The following image shows the components of a perceptron:\n",
    "\n",
    "![perceptron](https://user-images.githubusercontent.com/7232635/45839439-5c2ec580-bce2-11e8-9120-04084312d32e.png)\n",
    "\n",
    "A **perceptron** is a **linear classifier** that allows one to make predictions based on a linear predictor function combining a set of **weights** and **features** (inputs). Weights should be initialized to small random values. (small random values should be generated with a gaussian distribution having mean 0 and a standard deviation of 1. Then, multiply those values with $\\sqrt\\frac{2}{n_i}$ where $n_i$ is number of inputs for that layer). **Weights** and **inputs** will be combined with a dot product then summed with a **bias** term. The output of this combination will be passed through an **activation function** which will determine whether or not the perceptron will \"fire\" or not.\n",
    "\n",
    "$$ f(x) = \\begin{cases}\n",
    "               1 & \\text{if} & w \\cdot x + b > 0 \\\\\n",
    "               0 & \\text{otherwise}\n",
    "            \\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Used to determine whether a neuron should fire or not\n",
    "* Must be **nonlinear**\n",
    "\n",
    "### Choosing the Right Activation Function\n",
    "\n",
    "When choosing an activation function, determine when a neuron should fire (designate that the input is in-class). \n",
    "\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "Suppose we have a **binary classifier**, like cats vs dogs. We would like a nonlinear function that outputs values in the range $(0, 1)$. The sigmoid function could work as an activation function here.\n",
    "\n",
    "![sigmoid](https://user-images.githubusercontent.com/7232635/45764833-96746600-bc01-11e8-90e1-5f8942205ac5.png)\n",
    "\n",
    "One problem that may arise from using a sigmoid activation function is the **vanishing gradient problem**. Given that we would take the derivative of the sigmoid during back propagation, the derivative becomes very close to zero at $x$ values far from $0$. Tiny derivatives means that there will be tiny updates to the coefficients. This means that the neural network may learn at an **extremely slow rate**.\n",
    "\n",
    "#### ReLu\n",
    "\n",
    "To address the vanishing gradient problem, one can use the **rectified linear unit** function or **ReLu**. The ReLu function can be represented as $A(x) = max(0, x)$. \n",
    "\n",
    "![relu](https://user-images.githubusercontent.com/7232635/45765341-d4be5500-bc02-11e8-9666-e515cd5d1fa9.png)\n",
    "\n",
    "This function is nonlinear. The ReLu function is **unbounded** as it outputs values in the range $[0, inf)$. The unbounded nature of ReLu means that we can get very large values after performing the activation. Another problem is that the gradient could go to zero because it's value is zero with negative inputs.\n",
    "\n",
    "ReLu's do better than sigmoid in terms of computation as ReLu's are less computationally expensive.\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "ReLu's do well computationally, but when you are trying to output a set of classes in the **output layer**, it is better to use the **softmax** function. **Softmax** takes the outputs of the previous layer and turns them into probability values $[0, 1]$. These probability values sum up to $1$.\n",
    "\n",
    "![softmax](https://user-images.githubusercontent.com/7232635/45766352-58794100-bc05-11e8-907c-c97028463259.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions should be Non-Linear\n",
    "\n",
    "Why must the activation functions be nonlinear? Suppose the activation function is linear. Recall, a linear function can be represented as:\n",
    "\n",
    "$$ y = c \\cdot x $$\n",
    "\n",
    "When we perform gradient descent, we would take the derivative of this function and get a constant, $c$. The derivative does not depend on the input $x$ in this case. Therefore, any change made by back propagation is constant.\n",
    "\n",
    "What if we added more layers to the network with linear activation functions $f_n$? The neural network is more complex, so maybe we could learn more complex patterns?\n",
    "\n",
    "![linear_activation](https://user-images.githubusercontent.com/7232635/45762954-865a8780-bbfd-11e8-8c08-8f4da50daa75.png)\n",
    "\n",
    "No, you would not be able to learn more complex patterns! Even if more layers were added to the network with linear activation functions, the activation of the last layer would just be a combination of linear functions from the previous layers:\n",
    "\n",
    "$$ f_{output} = f_1(x_i, w) + f_2(x_i, w) + f_3(x_i, w) + f_4(x_i, w) + f_5(x_i, w) + f_6(x_i, w) + f_7(x_i, w) + f_8(x_i, w) + f_9(x_i, w) $$\n",
    "\n",
    "_A combination of linear functions is linear._ Therefore, we can represent the above combination with a new linear function, $g(x_i, w)$, where $g(x_i, w) = f_1(x_i, w) + f_2(x_i, w) + f_3(x_i, w) + f_4(x_i, w) + f_5(x_i, w) + f_6(x_i, w) + f_7(x_i, w) + f_8(x_i, w) + f_9(x_i, w)$\n",
    "\n",
    "This combination of linear functions can be replaced with a **single linear layer** with activation function, $g(x_i, w)$:\n",
    "\n",
    "![single_node](https://user-images.githubusercontent.com/7232635/45763762-2c5ac180-bbff-11e8-9b5c-8d0722b343bd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling XOR with a Neural Network Intuition\n",
    "\n",
    "Let's train a neural network to behave as an XOR operation given $2$ inputs.\n",
    "\n",
    "Recall, **XOR** can be represented by a truth table:\n",
    "\n",
    "| input $(x_1, x_2)$ | output $y$ |\n",
    "|:------| :------|\n",
    "| 0, 0  | 0 |\n",
    "| 0, 1  | 1 |\n",
    "| 1, 0  | 1 |\n",
    "| 1, 1  | 0 |\n",
    "\n",
    "The neural network will have $2$ inputs and $1$ output. But how will we transform the $2$ inputs to the correct output? Is one perceptron enough? Let's take a look at the plot of XOR. The green x's represent an output of $1$ and the red o's represent an output of $0$:\n",
    "\n",
    "![xor](https://user-images.githubusercontent.com/7232635/45830956-254db500-bccc-11e8-8978-282c6d15c0bf.png)\n",
    "\n",
    "To use $1$ perceptron, the points should be linearly separable with one line. In this case, you cannot separate the x's and o's with a single line:\n",
    "\n",
    "![xor_oneline](https://user-images.githubusercontent.com/7232635/45839897-96e52d80-bce3-11e8-8f33-5a2b03bd0019.png)\n",
    "\n",
    "Ignoring the x's and o's representation and focusing on just the class division between the points, is there a boolean logic operator that can represent this relation?\n",
    "\n",
    "Looking at the truth table of this relation we have:\n",
    "\n",
    "| input $(x_1, x_2)$ | output $y$ |\n",
    "|:------| :------|\n",
    "| 0, 0  | 1 |\n",
    "| 0, 1  | 1 |\n",
    "| 1, 0  | 1 |\n",
    "| 1, 1  | 0 |\n",
    "\n",
    "This truth table represents the **NAND** operation!\n",
    "\n",
    "Since we can not use a single line to separate all the x's and o's. How can we address this? We can use a 2nd perceptron, which will allow us to model another relation:\n",
    "\n",
    "![xor_oneline2](https://user-images.githubusercontent.com/7232635/45838955-fa219080-bce0-11e8-8620-b722790b587f.png)\n",
    "\n",
    "Looking at the truth table of this relation we have:\n",
    "\n",
    "| input $(x_1, x_2)$ | output $y$ |\n",
    "|:------| :------|\n",
    "| 0, 0  | 0 |\n",
    "| 0, 1  | 1 |\n",
    "| 1, 0  | 1 |\n",
    "| 1, 1  | 1 |\n",
    "\n",
    "This truth table represents the **OR** operation!\n",
    "\n",
    "Given that we need $2$ lines to separate the x's and o's, how can we combine the outputs of the **NAND** and **OR** operations to create **XOR**?\n",
    "\n",
    "We will need a 3rd perceptron to combine the outputs from perceptron 1 (**NAND**) and perceptron 2 (**OR**). Let's look at the current outputs of **NAND** and **OR** and the desired output for **XOR** as a truth table:\n",
    "\n",
    "| input $(x_1, x_2)$ | **NAND** output | **OR** output | **XOR** |\n",
    "|:------| :------| :------| :------|\n",
    "| 0, 0  | 1 | 0 | 0 |\n",
    "| 0, 1  | 1 | 1 | 1 |\n",
    "| 1, 0  | 1 | 1 | 1 |\n",
    "| 1, 1  | 0 | 1 | 0 |\n",
    "\n",
    "We can combine **NAND** and **OR** with **AND** to get **XOR**! Therefore, we can represent the 3rd perceptron as **AND**:\n",
    "\n",
    "![xor_twoline](https://user-images.githubusercontent.com/7232635/45840541-30f9a580-bce5-11e8-90ba-ce4c674112e8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying a Neural Network Modelling XOR with a Forward Pass\n",
    "\n",
    "Okay, so we understand how to represent **XOR** graphically, but how can we represent **XOR** with perceptrons in a neural network? **Remember a perceptron will be activated and output $1$ when $w \\cdot x + b > 0$ and $0$ otherwise.** This function represents a **step function**, which will be our **activation function** for this neural network.\n",
    "\n",
    "Suppose we have a neural network that we believe models **XOR** with 3 perceptrons. We want to verify that the randomly chosen weights, $w$, and biases, $b$, correctly model this boolean operator. The 2 nodes on the left represent our inputs $(x_1, x_2)$ which can be either $1$ or $0$. The right 3 nodes are our perceptrons, the **NAND**, **OR**, and **AND**. The weights, $w$ are labeled on the connections between nodes. Let the bias, $b$, be the values in the 3 perceptrons $(-.5, 1.5, -1.5)$:\n",
    "\n",
    "![xor_answer](https://user-images.githubusercontent.com/7232635/45900328-7386b500-bdad-11e8-9b7a-df8927db2ba1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at all combinations of inputs $(x_1, x_2)$ to verify our results. We will do a **forward pass** in the network and evaluate each of the 3 perceptrons.\n",
    "\n",
    "A **forward pass** calculates for each node:\n",
    "\n",
    "$$ z = \\sum_{i=1}^m(w_i \\cdot x_i) + b $$\n",
    "\n",
    "Then applies an activation function $f$ (in this case the step function) to our output from above to get an activation output, $a$:\n",
    "\n",
    "$$ a = f(z) $$\n",
    "\n",
    "1. Given $b = -.5$ (**OR** gate perceptron):\n",
    "\n",
    "| input $(x_1, x_2)$ | calculation $x_1 \\cdot w_1 + x_2 \\cdot w_2 + b$  | output $y$|\n",
    "|:-------------------|:----------------------------------------|:--------|\n",
    "| $(1, 1)$ | $$ 1 \\cdot 1 + 1 \\cdot 1 - 0.5 = 1.5$$ | $1$ |\n",
    "| $(1, 0)$ | $$ 1 \\cdot 1 + 0 \\cdot 1 - 0.5 = .5$$  | $1$ |\n",
    "| $(0, 1)$ | $$ 0 \\cdot 1 + 1 \\cdot 1 - 0.5 = .5$$  | $1$ |\n",
    "| $(0, 0)$ | $$ 0 \\cdot 1 + 0 \\cdot 1 - 0.5 = -.5$$ | $0$ |\n",
    "\n",
    "This agrees with our truth table for **OR** from above, so we know that the weights and bias associated with the **OR** perceptron is correct.\n",
    "\n",
    "2. Given $b = 1.5$ (**NAND** gate perceptron):\n",
    "\n",
    "| input $(x_1, x_2)$ | calculation $x_1 \\cdot w_1 + x_2 \\cdot w_2 + b$  | output $y$|\n",
    "|:-------------------|:----------------------------------------|:--------|\n",
    "| $(1, 1)$ | $$ 1 \\cdot -1 + 1 \\cdot -1 + 1.5 = -.5$$ | $0$ |\n",
    "| $(1, 0)$ | $$ 1 \\cdot -1 + 0 \\cdot -1 + 1.5 = .5$$  | $1$ |\n",
    "| $(0, 1)$ | $$ 0 \\cdot -1 + 1 \\cdot -1 + 1.5 = .5$$  | $1$ |\n",
    "| $(0, 0)$ | $$ 0 \\cdot -1 + 0 \\cdot -1 + 1.5 = 1.5$$ | $1$ |\n",
    "\n",
    "3. Given $b = -1.5$ (**AND** gate perceptron), using the output from the **OR** and **NAND** gates:\n",
    "\n",
    "|input (x_1, x_2) | hidden $(h_1, h_2)$ | calculation $x_1 \\cdot w_1 + x_2 \\cdot w_2 + b$  | output $y$|\n",
    "|:-------------------|:-------------------|:----------------------------------------|:--------|\n",
    "| $(1, 1)$ | $(1, 0)$ | $$ 1 \\cdot 1 + 0 \\cdot 1 - 1.5 = -.5$$ | $0$ |\n",
    "| $(1, 0)$ | $(1, 1)$ | $$ 1 \\cdot 1 + 1 \\cdot 1 - 1.5 = .5$$  | $1$ |\n",
    "| $(0, 1)$ | $(1, 1)$ | $$ 1 \\cdot 1 + 1 \\cdot 1 - 1.5 = .5$$  | $1$ |\n",
    "| $(0, 0)$ | $(0, 1)$ | $$ 0 \\cdot 1 + 1 \\cdot 1 - 1.5 = -.5$$ | $0$ |\n",
    "\n",
    "Comparing our outputs and the expected output of **XOR**, we get exactly what we would expect for **XOR**!\n",
    "\n",
    "But what if we didn't happen to guess the correct weights and bias units for our network initially? We can use **backpropagation** to _propagate_ the error from output back to the first hidden layer. That way, we can use this error to update the weights and biases. Updating the weights and biases will allow us to do forward propagation and hopefully improve on our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation allows us to understand how changing the weights and biases in a network will affect the cost function. Backpropagation is similar to gradient descent, but does not use a learning rate.\n",
    "\n",
    "The goal of backpropagation is to calculate the partial derivatives $\\frac{\\partial C}{\\partial w}$ and $\\frac{\\partial C}{\\partial b}$ for every $w$ and $b$. For backpropagation to be applied, the cost function, $C$, must be able to be written as an **average**.\n",
    "\n",
    "$$ C = \\frac{1}{n}\\sum_x C_x $$\n",
    "\n",
    "where $C_x$ is the cost function for a single training example, $x$. For example, for a single training example, we could have:\n",
    "\n",
    "$$C_x = \\frac{1}{2}||y - a^L||^2$$ where $a$ is the resulting value after having applied the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Steps in a Neural Network\n",
    "\n",
    "1. Model Initialization\n",
    "\n",
    "Commonly, use a **random** initialization of weights and biases.\n",
    "\n",
    "2. Feed Forward Propagation\n",
    "\n",
    "Determine the performance of the model. Given the inputs, we calculate the output of the model by performing the dot product between the weights and inputs, applying the activation function, and feeding this output to the next layers until reaching a final output.\n",
    "\n",
    "3. Loss function\n",
    "\n",
    "We define a loss function to use for our neural network to use as a metric for the performance of our network. The loss function should be written as an average, since we are calculating the loss or cost given the outputs of all the nodes in our network.\n",
    "\n",
    "4. Compute the gradient of the loss function\n",
    "\n",
    "5. Backpropagation\n",
    "\n",
    "Backpropagate the error\n",
    "\n",
    "6. Weight update\n",
    "\n",
    "7. Iterate until convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network in Code\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs\n",
    "\n",
    "[Stanford CNN class](http://cs231n.stanford.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
